---
title: "Exploratory Analysis for MoTR Reading Data"
output: html_notebook
---

```{r}
shhh <- suppressPackageStartupMessages # It's a library, so shhh!

shhh(library( mgcv ))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lme4))
shhh(library(tidymv))
shhh(library(gamlss))
shhh(library(gsubfn))
shhh(library(lmerTest))
shhh(library(tidyverse))
shhh(library(boot))
shhh(library(rsample))
shhh(library(plotrix))
shhh(library(ggrepel))
shhh(library(mgcv))

shhh(library(brms))
shhh(library(bayesplot))
shhh(library(patchwork))
shhh(library(MASS))
shhh(library(tidyr))
shhh(library(extraDistr))
shhh(library(purrr))
# For exercises with Stan code
shhh(library(rstan))
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = FALSE)

shhh(library(car))
shhh(library(coda))
shhh(library(gridExtra))
shhh(library(posterior))

theme_set(theme_bw())
options(digits=4)
options(scipen=999)
set.seed(444)
pipe_message = function(.data, status) {message(status); .data}

```


# Read in MoTR Data

```{r}

rate = 160

file_prefix = "../reading_measures/provo/provo_f160/"
fnames = list.files(path=file_prefix)

df = data.frame()
for (f in fnames) {
  temp = read.csv(paste0(file_prefix, "/", f)) %>%
    mutate(subj = str_remove(f, "_reading_measures.csv"))
  df = rbind(df, temp)
}

# Filter out readers who don't answer the comprehension questions correctly
filter_df = df %>%
  group_by(para_nr, subj) %>% summarise(correct = if_else(unique(correctness) == 1, 1, 0)) %>% ungroup() %>%
  drop_na() %>%
  group_by(subj) %>% summarise(p_correct = mean(correct)) %>% ungroup() %>%
  mutate(p_correct = round(p_correct, digits = 2))

filter_df = filter_df %>% filter(p_correct < 0.8)
filter_list = filter_df$subj
## reader_3:0.70, reader_60:0.79, reader_76:0.72 , reader_256:0.71 , reader_262:0.57 

raw_df = df %>%
  filter(! subj %in% c(filter_list)) %>%
  mutate(word = str_trim(word)) %>%
  mutate(subj = str_remove(subj, "reader_")) %>%
  mutate(subj = as.character(subj)) %>%
  # filter(! subj %in% c("3", "60", "76", "256", "262")) %>% # Explanation for this filtering
  mutate(FPReg = if_else(total_duration == 0, -1, FPReg)) %>% #If the word is skipped we can't say that it wasn't regressed on the first pass. Set to a "NA"
  dplyr::select(expr_id, cond_id, para_nr, word, word_nr, first_duration, total_duration, gaze_duration, go_past_time, FPReg, subj)

length(unique(raw_df$subj))

df %>%
  filter(! subj %in% c(filter_list)) %>%
  filter(FPReg >= 0) %>%
  dplyr::select(FPReg) %>%
  drop_na() %>%
  summarise( m = mean(FPReg))

df %>%
  filter(! subj %in% c(filter_list)) %>%
  dplyr::select(FPFix) %>%
  drop_na() %>%
  summarise( m = mean(FPFix))


```


```{r}
# Average across subjects
motr_agg_df = raw_df %>%
  gather(metric, value, 6:10) %>%
    filter(value >= 0) %>% #Removes the "NA" values for FPReg
  
    # ==== Remove skipped words
    # mutate(zero = if_else(metric != "FPReg" & value == 0,T, F)) %>%
    # filter(zero == F) %>%
  
    drop_na() %>%
    group_by(para_nr, word_nr, word, metric) %>% 
      mutate(outlier = if_else(metric != "FPReg" & value > (mean(value) + 3 * sd(value)), T, F)) %>% filter(outlier == F) %>%
      summarise(value = mean(value), nsubj = length(unique(subj))) %>%
  ungroup() %>%
  arrange(para_nr, word_nr) %>%
  rename(text_id = para_nr, word_text_idx = word_nr, motr_value = value)

# View(motr_agg_df)
# write.csv(motr_agg_df, file = "/Users/cui/Desktop/MoTR/pipeline/ancillary_data/motr_agg_df.csv", row.names = FALSE)

```




# Comparison to Provo


```{r}
# Read in Provo surprisal, frequency and length data
provo_modeling_df = read.csv("../ancillary_data/provo_df.csv") %>%
  dplyr::select(text_id, sent_id, trigger_idx, word, freq, surp, len) %>%
  rename(word_idx = trigger_idx)

provo_modeling_df
# View(provo_modeling_df)

```

```{r}
# Read in Provo eyetracking data

provo_raw_df = read.csv("../ancillary_data/provo_eyetracking.csv")

```

```{r}

# unique(provo_raw_df$Participant_ID)
# length(unique(provo_raw_df$Participant_ID))

provo_eyetracking_df = provo_raw_df %>%
  dplyr::select(Participant_ID, Text_ID, Sentence_Number, Word_In_Sentence_Number, Word, Word_Number, IA_FIRST_FIX_PROGRESSIVE, IA_FIRST_RUN_DWELL_TIME, IA_DWELL_TIME, IA_REGRESSION_PATH_DURATION, IA_REGRESSION_OUT, IA_SKIP) %>%
  rename( #first_duration = IA_FIRST_FIXATION_DURATION,   
          gaze_duration = IA_FIRST_RUN_DWELL_TIME,
          total_duration = IA_DWELL_TIME,
          go_past_time = IA_REGRESSION_PATH_DURATION,
          subj = Participant_ID,
          text_id = Text_ID,
          sent_id = Sentence_Number,
          word_idx = Word_In_Sentence_Number,
          word_text_idx = Word_Number,   # IA_ID?
          word = Word,      # Word?
          FPReg = IA_REGRESSION_OUT,
          skip = IA_SKIP,
          ff_progressive = IA_FIRST_FIX_PROGRESSIVE) %>%
  mutate(first_duration = gaze_duration) %>%
  mutate(gaze_duration = if_else(ff_progressive == 0, 0, as.double(gaze_duration)),
         go_past_time = if_else(ff_progressive == 0, 0, as.double(go_past_time))) %>%
  dplyr::select(-ff_progressive) %>%
  
  mutate(
    gaze_duration = if_else(total_duration == 0, 0, as.double(gaze_duration)),
      go_past_time = if_else(total_duration == 0, 0, as.double(go_past_time)),
      FPReg = if_else(total_duration == 0, -1, as.double(FPReg)),
      first_duration =  if_else(total_duration == 0, 0, as.double(first_duration)),
  ) %>%
  
  # drop_na() %>%     # will drop the whole row with all the metrics
  gather(metric, value, 7:12) %>%
  filter(value >= 0) %>%          # filter skipped word in eye tracking data for FPReg
  # ==== Remove skipped words
  # mutate(zero = if_else(metric != "FPReg" & value == 0,T, F)) %>%
  # filter(zero == F) %>%
  
  # mutate(value = if_else(is.na(value), as.integer(0), as.integer(value))) %>%
  # mutate(value = if_else(metric != "FPReg" & is.na(value), as.integer(0), as.integer(value))) %>%
  drop_na() %>%
  mutate(word = str_trim(word)) %>%
  mutate(subj = str_remove(subj, "Sub")) %>%
  mutate(subj = as.integer(subj)) %>%
    group_by(text_id, word_text_idx, sent_id, word_idx, word, metric) %>%
    mutate(outlier = if_else(metric != "FPReg" & metric != "skip" & value > (mean(value) + 3 * sd(value) ), T, F)) %>%
    filter(outlier == F) %>%
  ungroup() #%>%

# Aggregate cross-participant data for all subjects
provo_eyetracking_agg_df = provo_eyetracking_df %>%
  group_by(text_id, word_text_idx, sent_id, word_idx, word, metric) %>%
    summarise(value = mean(value),
              nsubj = length(unique(subj))) %>%
    ungroup()

# View(provo_eyetracking_df)

# View(provo_eyetracking_agg_df)
# write.csv(provo_eyetracking_agg_df, file = "/Users/cui/Desktop/MoTR/pipeline/ancillary_data/provo_eyetracking_agg_df.csv", row.names = FALSE)

provo_raw_df %>%
  dplyr::select(IA_REGRESSION_OUT) %>%
  drop_na() %>%
  summarise( m = mean(IA_REGRESSION_OUT))

provo_raw_df %>%
  dplyr::select(IA_SKIP) %>%
  drop_na() %>%
  summarise( m = mean(IA_SKIP))


```

```{r}

# Split the eyetracking data in two by subjects to see how well it correlates with itself
provo_eyetracking_subj1_df_temp = provo_eyetracking_df %>%
  filter(subj <= 42) %>%
  mutate(word_text_idx = as.integer(word_text_idx - 1)) %>%
  group_by(text_id, word_text_idx, sent_id, word_idx, word, metric) %>%
    summarise(value = mean(value)) %>%
  ungroup() %>%
  rename(value_1 = value) #%>%
  # dplyr::select(-sent_id, -word_idx)

# View(provo_eyetracking_subj1_df_temp)

provo_eyetracking_subj1_df = merge(provo_eyetracking_subj1_df_temp, motr_agg_df, by=c("text_id", "word_text_idx", "metric")) %>%
  arrange(text_id, sent_id, word_idx) %>%
  filter(!(text_id == 13 & word_text_idx >= 20 & word_text_idx <= 52)) %>%
  filter(!(text_id == 3 & word_text_idx >= 46 & word_text_idx <= 57)) %>%
  rename(word = word.y) %>%
  dplyr::select(text_id, word_text_idx, metric, word, value_1)

# View(provo_eyetracking_subj1_df)

provo_eyetracking_subj2_df = provo_eyetracking_df %>%
  filter(subj > 42) %>%
  mutate(word_text_idx = as.integer(word_text_idx - 1)) %>%
  group_by(text_id, word_text_idx, sent_id, word_idx, word, metric) %>%
    summarise(value = mean(value)) %>%
  ungroup() %>%
    rename(value_2 = value)%>%
  dplyr::select(-sent_id, -word_idx)

# View(provo_eyetracking_subj2_df)
  
provo_eyetr_grouped_df = merge(provo_eyetracking_subj2_df, provo_eyetracking_subj1_df, by=c("text_id", "word_text_idx", "metric")) %>%
  # filter(word.x == word.y) %>%
  dplyr::select(-word.y) %>%
  group_by(metric) %>%
    mutate(motr_outlier = if_else(metric != "FPReg" & metric != "skip" & value_1 > (mean(value_1) + 3 * sd(value_1) ), T, F)) %>%
    filter(motr_outlier == F) %>%
    mutate(eyetr_outlier = if_else(metric != "FPReg" & metric != "skip" & value_2 > (mean(value_2) + 3 * sd(value_2) ), T, F)) %>%
    filter(eyetr_outlier == F) %>%
  ungroup() %>%
  gather(measure, value, c("value_1", "value_2")) %>%
  dplyr::select(-motr_outlier, -eyetr_outlier)

# View(provo_eyetr_grouped_df)

```


```{r}
provo_df = merge(provo_eyetracking_agg_df, provo_modeling_df, by=c("text_id", "sent_id", "word_idx")) %>%
  mutate(word_text_idx = as.integer(word_text_idx - 1)) %>%
  arrange(text_id, sent_id, word_idx) %>%
  rename(eyetr_value = value) 

provo_df = merge(provo_df, motr_agg_df, by=c("text_id", "word_text_idx", "metric")) %>%
arrange(text_id, sent_id, word_idx) %>%
  # almost all the word.x != word.y is because of normalization problem, so we can keep them, instead, deleting some special cases
filter(!(text_id == 13 & word_text_idx >= 20 & word_text_idx <= 52)) %>%
  filter(!(text_id == 3 & word_text_idx >= 46 & word_text_idx <= 57)) %>%
# filter(word.x == word) #%>%
dplyr::select(-word.x, -word.y) %>%
group_by(metric) %>%
  mutate(motr_outlier = if_else(metric != "FPReg" & motr_value > (mean(motr_value) + 3 * sd(motr_value) ), T, F)) %>%
  filter(motr_outlier == F) %>%
  mutate(eyetr_outlier = if_else(metric != "FPReg" & eyetr_value > (mean(eyetr_value) + 3 * sd(eyetr_value) ), T, F)) %>%
  filter(eyetr_outlier == F) %>%
ungroup() %>%
gather(measure, value, c("eyetr_value", "motr_value")) %>%
dplyr::select(-motr_outlier, -eyetr_outlier)
  
# View(provo_df)
# provo_df
```


# Bayesian -- brm & gams -- surprisal & RTs
```{r}
# View(provo_df)

gam_modeling_df = provo_df %>%
  spread(measure, value) %>%
  # mutate(len = nchar(word)) %>%  # len has already exists, but do not count punct into len.
  group_by(metric, sent_id, text_id) %>%
    arrange(word_text_idx) %>%
    mutate(prev_surp = lag(surp),
           prev2_surp = lag(prev_surp),
           prev_freq = lag(freq),
           prev2_freq = lag(prev_freq),
           prev_len = lag(len),
           prev2_len = lag(prev_len),
           prev_eyetr_value = lag(eyetr_value)) %>%
  ungroup() %>%
  drop_na() %>%
  rename(psychometric = motr_value) %>%
  filter(psychometric > 0)
# View(gam_modeling_df)

gam_modeling_df %>%
  filter(metric == "gaze_duration") %>%
  arrange(text_id, sent_id, word_idx) 

gam_modeling_df %>% 
  ggplot(aes(x = psychometric)) +
  geom_density() +
  facet_wrap(~metric, scales = "free") +
  theme_bw() +
  scale_fill_brewer(palette = "Set1")

```

## Shape of surprisal / RT relationship
```{r}
## normal distr
# priors <- c(
#   prior(normal(180, 50), class = Intercept),
#   prior(normal(0, 20), class = b),
#   prior(normal(0, 20), class = sds),
#   prior(normal(0, 50), class = sigma)
# )

## log normal distr
priors <- c(
  prior(normal(5, 0.1), class = Intercept),
  prior(normal(0, 0.5), class = b),
  prior(normal(0, 2), class = sds),
  prior(normal(0, 1), class = sigma)
)

## gamma distr
priors_gamma <- c(
  prior(normal(5.5, 1), class = Intercept),
  prior(normal(0, 0.5), class = b),
  prior(cauchy(0, 0.5), class = sds),
  prior(exponential(3), class = shape)
)

fit_gam_inner = function(bootstrap_sample, mean_predictors, target_prev_word, metric) {
  df = analysis(bootstrap_sample)
  # df = bootstrap_sample$data

  # m = gam(psychometric ~ s(surp, bs = 'cr', k = 20) + s(prev_surp, bs = 'cr', k = 20) + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, method = 'REML')
  m <- brm(
    psychometric ~ s(surp, bs = 'cr', k = 6) + s(prev_surp, bs = 'cr', k = 6) + t2(freq, len, bs = 'cr') + t2(prev_freq, prev_len, bs = 'cr'),
    data = df,
    # family = gaussian(),
    # family = lognormal(),
    # prior = priors,
    family = Gamma(link = "log"),
    prior = priors_gamma,
    cores = 8,
    seed = 444,
    chain = 4,
    iter = 4000,
    file = paste0("./bayesian_models_surp/drop0s/gamma2_provo_", metric, "_", bootstrap_sample$id),
    # file = paste0("./bayesian_models_surp/with0s/gamma_provo_", metric, "_", bootstrap_sample$id),
    backend = "cmdstanr",
    control = list(adapt_delta = 0.99)
    )
  
  if(target_prev_word == F) {
    newdata = data.frame(surp=seq(0,20,by=0.1), prev_surp=mean_predictors$surp,
                         freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                         len=mean_predictors$len, prev_len=mean_predictors$len)

    all_term_predictions = predict(m, newdata=newdata, summary = TRUE)#[, "Estimate"] - pred_mean[1, "Estimate"]
    # View(all_term_predictions)

  } else {
    newdata = data.frame(surp=mean_predictors$surp, prev_surp=seq(0,20,by=0.1),
                         freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                         len=mean_predictors$len, prev_len=mean_predictors$len)

    all_term_predictions = predict(m, newdata=newdata, summary = TRUE)#[, "Estimate"] - pred_mean[1, "Estimate"]
    # View(all_term_predictions)
  }


    predictions = all_term_predictions[, "Estimate"] #- pred_mean[1, "Estimate"]
  
  # print(class(predictions))
  # print(dim(predictions))
  
  return( newdata %>% mutate(y = predictions))
}

fit_gam = function(df, mean_predictors, target_prev_word, metric, alpha=0.05) {
  # Bootstrap-resample data

  boot_models = df %>% bootstraps(times=17) %>% 
   # Fit a GAM and get predictions for each sample
    mutate(smoothed=map(splits, fit_gam_inner, mean_predictors=mean_predictors, target_prev_word=target_prev_word, metric=metric))
  
  # Extract mean and 5% and 95% percentile y-values for each surprisal value
  if(target_prev_word == F) {
    
    result = boot_models %>% 
      unnest(smoothed) %>% 
      dplyr::select(surp, y) %>%
      group_by(surp) %>%
        summarise(y_lower=quantile(y, alpha / 2), 
                  y_upper=quantile(y, 1 - alpha / 2),
                  y=mean(y)) %>% 
      ungroup()
  } else {
    
    result = boot_models %>% 
      unnest(smoothed) %>% 
      dplyr::select(prev_surp, y) %>%
      group_by(prev_surp) %>%
        summarise(y_lower=quantile(y, alpha / 2), 
                  y_upper=quantile(y, 1 - alpha / 2),
                  y=mean(y)) %>%
      ungroup() %>%
      rename(surp = prev_surp)
  }
  # View(result)
  return (result)
}

```



```{r, eval=FALSE}

smooths_df = data.frame()

metrics = c("gaze_duration", "total_duration", "go_past_time")
# metrics = c("total_duration")
for (m in metrics) {
  for( tval in c(T,F)) {
    print(paste0("Fitting model for ", m))
    dummy_df = gam_modeling_df %>% filter(metric == m) %>%
      mutate(psychometric =  pmax(psychometric, 1))
    # View(dummy_df)
    mean_predictors = dummy_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
    # View(mean_predictors)
    smooths = dummy_df %>% fit_gam(., mean_predictors, target_prev_word = tval, metric = m)
    #Fix 0 surprisal = 0 mse
    gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
    smooths_df = rbind(smooths_df, gam_smooths %>% mutate(psychometric = m, prev_word = tval))
    # View(smooths_df)
  }
}

```


### Get Density Data
```{r}
get_d_points = function(df) {
    x = density(df$surp)$x
    y = density(df$surp)$y
    return(data.frame(x, y))
  }

density_data = data.frame()

for(m in c("gaze_duration", "total_duration", "go_past_time")) {
  dummy_df = provo_df %>% filter(metric == m) %>%
      do({get_d_points(.)}) %>%
      filter(x>0, x<20)
  density_data = rbind(density_data, dummy_df %>% mutate(metric=m))
}

```


```{r}

plotting_df = smooths_df %>%
  mutate(target_word = if_else(prev_word == F, "wt", "wt-1"))
# write.csv(plotting_df, "gamma_provo_surprisal_bayesian_drop0s.csv", row.names = FALSE)

vnames <-list(
  "gaze_duration" = "Gaze Duration",
  "go_past_time" = "Go Past Time",
  "total_duration" = "Total Duration",
  'wt-1' = bquote(w[t-1]),
  'wt' = bquote(w[t])
)

vlabeller <- function(variable,value){
  return(vnames[value])
}

# Surprisal curves
  ggplot() +
      # Surrp / Rt data
      # geom_line(data = smooths_df, aes(x=prev_surp, y=y, color = psychometric), size=0.7) +
      geom_line(data = plotting_df, aes(x=surp, y=y, color = psychometric, linetype=target_word),  size=0.7) +
      # geom_ribbon(data = smooths_df, aes(x=prev_surp, ymin=y_lower, ymax=y_upper, fill = psychometric), alpha=0.3, size=0.5) +
      geom_ribbon(data = plotting_df, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = psychometric), alpha=0.3, size=0.5) +
      # Density Data
      annotate("rect", xmin=-5, xmax=25, ymin=-25,ymax=-13, fill="white", color="grey", alpha=1, size = 0) +
      geom_line(data = density_data, aes(x=x, y=y*100 - 25), size = 0.2, color="#aaaaaa") +

      geom_ribbon(data = density_data, aes(x=x, ymin=-50, ymax=y*100 - 25), color="#dadeea", alpha = 0.1) +

      geom_hline(yintercept = -13, color = "black", size = 0.1) +
    
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
      facet_wrap(psychometric~target_word, nrow = 1, labeller = vlabeller) +
      ylab("Slowdown due to Surprisal (ms)") +
      xlab("Surprisal of Word") +
      coord_cartesian(ylim = c(-20, 100), xlim = c(0, 20)) +
      # ggtitle("MoTR Times and Previous Word Surprisal")
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank()
  )
  
# ggsave("../visualization/paper/surp_rt_wt_gamma_drop0s.pdf", device = "pdf", width = 6, height = 2.5)


```

# plotting for visualization -- exploratory
```{r}
custom_histogram_plot <- function(model, newdata) {
  # Predict on the new data without summarizing
  # View(newdata)
  predictions <- predict(model, newdata = newdata, summary = FALSE)
  
  # Calculate the mean of the predictions for each data point
  mean_predictions <- colMeans(as.matrix(predictions))
  mean_predictions_df <- mean_predictions %>% as.data.frame()
  View(mean_predictions_df)
  
  # Create a histogram plot
  ggplot(data.frame(MeanPrediction = mean_predictions), aes(x = MeanPrediction)) +
    geom_histogram(fill = "blue", alpha = 0.5, bins = 30) +
    labs(title = "Histogram Distribution of Mean Predictions",
         x = "Mean Prediction", y = "Frequency") +
    theme_minimal()
}

custom_density_plot <- function(model, newdata) {
  # Predict on the new data without summarizing
  all_term_predictions = predict(model, newdata=newdata, summary = FALSE)
  first_column_matrix <- matrix(all_term_predictions[, 1], nrow = nrow(all_term_predictions), ncol = ncol(all_term_predictions), byrow = FALSE)
  # Subtract the first_column_matrix from the original array
  predictions <- all_term_predictions - first_column_matrix
  
  # Calculate the mean of the predictions for each data point
  mean_predictions <- colMeans(as.matrix(predictions))
  mean_predictions_df <- mean_predictions %>% as.data.frame()
  View(mean_predictions_df)
  # Create a histogram plot
  ggplot(data.frame(MeanPrediction = mean_predictions), aes(x = MeanPrediction)) +
    geom_density(color = "red") +
    labs(title = "Density Distribution of Mean Predictions",
         x = "Mean Prediction", y = "Density") +
    theme_minimal()
}

custom_density_plot_2 <- function(predictions) {
  # Convert predictions to a matrix
  predictions_matrix <- as.matrix(predictions)

  # Create a list to store the individual plots
  plots <- list()

  # Create individual density plots for every 40th column
  for (i in seq(40, 200, by = 40)) {
    p <- ggplot(data.frame(Value = predictions_matrix[, i]), aes(x = Value)) +
      geom_density(color = "red") +
      labs(title = paste("Density Distribution of Column", i),
           x = "Value", y = "Density") +
      theme_minimal()
    plots[[i / 40]] <- p
  }

  return(plots)
}
```

# try to use a single model
```{r}
gd = read_rds("./bayesian_models_surp/drop0s/gamma2_provo_gaze_duration_Bootstrap12.rds")
td = read_rds("./bayesian_models_surp/drop0s/gamma2_provo_total_duration_Bootstrap12.rds")
gpt = read_rds("./bayesian_models_surp/drop0s/gamma2_provo_go_past_time_Bootstrap12.rds")

# models <- list(gaze_duration = gd, total_duration = td, go_past_time = gpt)
models <- list(gaze_duration = gd, total_duration = td, go_past_time = gpt)

smooths_df = data.frame()
for (model_name in names(models)){
  m <- models[[model_name]]

  for( target_prev_word in c(T,F)) {
    if(target_prev_word == F) {
        newdata = data.frame(surp=seq(0,20,by=0.1), prev_surp=mean_predictors$surp,
                             freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                             len=mean_predictors$len, prev_len=mean_predictors$len)
      } else {
        newdata = data.frame(surp=mean_predictors$surp, prev_surp=seq(0,20,by=0.1),
                             freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                             len=mean_predictors$len, prev_len=mean_predictors$len)
      }
    all_term_predictions = predict(m, newdata=newdata, summary = FALSE)
    
    # ============================================ bootstrap ==========================================
    means <- numeric(201)
    lower_crl<- numeric(201)
    upper_crl <- numeric(201)

    for (i in 1:201) {
      samples <- all_term_predictions[,i]
      bootstrap_means <- replicate(100, mean(sample(samples, replace=TRUE, size = 300)))
      means[i] <- mean(bootstrap_means)
      lower_crl[i] <- quantile(bootstrap_means, 0.025)
      upper_crl[i] <- quantile(bootstrap_means, 0.975)
    }
    
    # ==================================== divide posterior prediction into 20 folder ====================================
    # # Divide the transformed_predictions into 20 parts, calculate colMeans, and store in result
    # sub_mean_pred <- matrix(nrow = 20, ncol = 201)
    # for (i in 1:20) {
    #   subset_array <- all_term_predictions[((i - 1) * 400 + 1):(i * 400), ]
    #   sub_mean_pred[i, ] <- colMeans(subset_array)
    # }
    # 
    # # Convert result to a data frame
    # mean_pred_df <- as.data.frame(sub_mean_pred)
    # means <- colMeans(mean_pred_df)
    # lower_crl <- apply(mean_pred_df, 2, function(x) quantile(x, 0.025))
    # upper_crl <- apply(mean_pred_df, 2, function(x) quantile(x, 0.975))
    
    stat <- data.frame(
      y = means,
      y_lower = lower_crl,
      y_upper = upper_crl,
      prev_word = target_prev_word,
      psychometric = model_name
    )

    result <- cbind(newdata, stat) %>% 
      mutate(surp = if_else(prev_word == TRUE, prev_surp, surp)) %>%
      dplyr::select(surp, y, y_lower, y_upper, prev_word, psychometric) %>% 
      mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta) 
    # View(result)
    smooths_df = rbind(smooths_df, result)
  }
}
# View(smooths_df)
  
  
```


### Get Density Data
```{r}
get_d_points = function(df) {
    x = density(df$surp)$x
    y = density(df$surp)$y
    return(data.frame(x, y))
  }

density_data = data.frame()

for(m in c("gaze_duration", "total_duration", "go_past_time")) {
  dummy_df = provo_df %>% filter(metric == m) %>%
      do({get_d_points(.)}) %>%
      filter(x>0, x<20)
  density_data = rbind(density_data, dummy_df %>% mutate(metric=m))
}

```


```{r}

plotting_df = smooths_df %>%
  mutate(target_word = if_else(prev_word == F, "wt", "wt-1"))

# write.csv(plotting_df, "gamma_provo_surprisal_bayesian_one_model_no_bootstrap.csv", row.names = FALSE)

vnames <-list(
  "gaze_duration" = "Gaze Duration",
  "go_past_time" = "Go Past Time",
  "total_duration" = "Total Duration",
  'wt-1' = bquote(w[t-1]),
  'wt' = bquote(w[t])
)

vlabeller <- function(variable,value){
  return(vnames[value])
}

# Surprisal curves
  ggplot() +
      # Surrp / Rt data
      # geom_line(data = smooths_df, aes(x=prev_surp, y=y, color = psychometric), size=0.7) +
      geom_line(data = plotting_df, aes(x=surp, y=y, color = psychometric, linetype=target_word),  size=0.7) +
      # geom_ribbon(data = smooths_df, aes(x=prev_surp, ymin=y_lower, ymax=y_upper, fill = psychometric), alpha=0.3, size=0.5) +
      geom_ribbon(data = plotting_df, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = psychometric), alpha=0.3, size=0.5) +
      # Density Data
      annotate("rect", xmin=-5, xmax=25, ymin=-25,ymax=-13, fill="white", color="grey", alpha=1, size = 0) +
      geom_line(data = density_data, aes(x=x, y=y*100 - 25), size = 0.2, color="#aaaaaa") +

      geom_ribbon(data = density_data, aes(x=x, ymin=-50, ymax=y*100 - 25), color="#dadeea", alpha = 0.1) +

      geom_hline(yintercept = -13, color = "black", size = 0.1) +
    
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
      facet_wrap(psychometric~target_word, nrow = 1, labeller = vlabeller) +
      ylab("Slowdown due to Surprisal (ms)") +
      xlab("Surprisal of Word") +
      coord_cartesian(ylim = c(-20, 100), xlim = c(0, 20)) +
      # ggtitle("MoTR Times and Previous Word Surprisal")
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank()
  )
  
# ggsave("../visualization/paper/surp_rt_wt_gammaone_model_no_bootstrap.pdf", device = "pdf", width = 6, height = 2.5)

```




