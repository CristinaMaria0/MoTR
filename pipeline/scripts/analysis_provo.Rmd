---
title: "Exploratory Analysis for MoTR Reading Data"
output: html_notebook
---

```{r}
shhh <- suppressPackageStartupMessages # It's a library, so shhh!

shhh(library( mgcv ))
shhh(library(dplyr))
shhh(library(ggplot2))
shhh(library(lme4))
shhh(library(tidymv))
shhh(library(gamlss))
shhh(library(gsubfn))
shhh(library(lmerTest))
shhh(library(tidyverse))
shhh(library(boot))
shhh(library(rsample))
shhh(library(plotrix))
shhh(library(ggrepel))
shhh(library(mgcv))

theme_set(theme_bw())
options(digits=4)
options(scipen=999)
set.seed(444)
pipe_message = function(.data, status) {message(status); .data}

```


# Read in MoTR Data

```{r}

rate = 160

file_prefix = "../reading_measures/provo/provo_f160/"
fnames = list.files(path=file_prefix)

df = data.frame()
for (f in fnames) {
  temp = read.csv(paste0(file_prefix, "/", f)) %>%
    mutate(subj = str_remove(f, "_reading_measures.csv"))
  df = rbind(df, temp)
}

# Filter out readers who don't answer the comprehension questions correctly
filter_df = df %>%
  group_by(para_nr, subj) %>% summarise(correct = if_else(unique(correctness) == 1, 1, 0)) %>% ungroup() %>%
  drop_na() %>%
  group_by(subj) %>% summarise(p_correct = mean(correct)) %>% ungroup() %>%
  mutate(p_correct = round(p_correct, digits = 2))

filter_df = filter_df %>% filter(p_correct < 0.8)
filter_list = filter_df$subj
## reader_3:0.70, reader_60:0.79, reader_76:0.72 , reader_256:0.71 , reader_262:0.57 

raw_df = df %>%
  filter(! subj %in% c(filter_list)) %>%
  mutate(word = str_trim(word)) %>%
  mutate(subj = str_remove(subj, "reader_")) %>%
  mutate(subj = as.character(subj)) %>%
  #filter(! subj %in% c("3", "60", "76", "256", "262")) %>% # Explanation for this filtering
  mutate(FPReg = if_else(total_duration == 0, -1, FPReg)) %>% #If the word is skipped we can't say that it wasn't regressed on the first pass. Set to a "NA"
  dplyr::select(expr_id, cond_id, para_nr, word, word_nr, first_duration, total_duration, gaze_duration, go_past_time, FPReg, subj)

length(unique(raw_df$subj))

df %>%
  filter(! subj %in% c(filter_list)) %>%
  filter(FPReg >= 0) %>%
  dplyr::select(FPReg) %>%
  drop_na() %>%
  summarise( m = mean(FPReg))

df %>%
  filter(! subj %in% c(filter_list)) %>%
  dplyr::select(FPFix) %>%
  drop_na() %>%
  summarise( m = mean(FPFix))


```


```{r}
# Average across subjects
motr_agg_df = raw_df %>%
  gather(metric, value, 6:10) %>%
    filter(value >= 0) %>% #Removes the "NA" values for FPReg
  
    # ==== Remove skipped words
    #mutate(zero = if_else(metric != "FPReg" & value == 0,T, F)) %>%
    #filter(zero == F) %>%
  
    drop_na() %>%
    group_by(para_nr, word_nr, word, metric) %>% 
      mutate(outlier = if_else(value > (mean(value) + 3 * sd(value)), T, F)) %>% filter(outlier == F) %>%
      summarise(value = mean(value), nsubj = length(unique(subj))) %>%
  ungroup() %>%
  arrange(para_nr, word_nr) %>%
  rename(text_id = para_nr, word_text_idx = word_nr, motr_value = value)

# View(motr_agg_df)
# write.csv(motr_agg_df, file = "/Users/cui/Desktop/MoTR/pipeline/ancillary_data/motr_agg_df.csv", row.names = FALSE)

```




# Comparison to Provo


```{r}
# Read in Provo surprisal, frequency and length data
provo_modeling_df = read.csv("../ancillary_data/provo_df.csv") %>%
  dplyr::select(text_id, sent_id, trigger_idx, word, freq, surp, len) %>%
  rename(word_idx = trigger_idx)

provo_modeling_df
# View(provo_modeling_df)

```

```{r}
# Read in Provo eyetracking data

provo_raw_df = read.csv("../ancillary_data/provo_eyetracking.csv")

```

```{r}

# unique(provo_raw_df$Participant_ID)
# length(unique(provo_raw_df$Participant_ID))

provo_eyetracking_df = provo_raw_df %>%
  dplyr::select(Participant_ID, Text_ID, Sentence_Number, Word_In_Sentence_Number, Word, Word_Number, IA_FIRST_FIX_PROGRESSIVE, IA_FIRST_RUN_DWELL_TIME, IA_DWELL_TIME, IA_REGRESSION_PATH_DURATION, IA_REGRESSION_OUT, IA_SKIP) %>%
  rename( #first_duration = IA_FIRST_FIXATION_DURATION,   
          gaze_duration = IA_FIRST_RUN_DWELL_TIME,
          total_duration = IA_DWELL_TIME,
          go_past_time = IA_REGRESSION_PATH_DURATION,
          subj = Participant_ID,
          text_id = Text_ID,
          sent_id = Sentence_Number,
          word_idx = Word_In_Sentence_Number,
          word_text_idx = Word_Number,   # IA_ID?
          word = Word,      # Word?
          FPReg = IA_REGRESSION_OUT,
          skip = IA_SKIP,
          ff_progressive = IA_FIRST_FIX_PROGRESSIVE) %>%
  mutate(first_duration = gaze_duration) %>%
  mutate(gaze_duration = if_else(ff_progressive == 0, 0, as.double(gaze_duration)),
         go_past_time = if_else(ff_progressive == 0, 0, as.double(go_past_time))) %>%
  dplyr::select(-ff_progressive) %>%
  
  mutate(
    gaze_duration = if_else(total_duration == 0, 0, as.double(gaze_duration)),
      go_past_time = if_else(total_duration == 0, 0, as.double(go_past_time)),
      FPReg = if_else(total_duration == 0, -1, as.double(FPReg)),
      first_duration =  if_else(total_duration == 0, 0, as.double(first_duration)),
  ) %>%
  
  # drop_na() %>%     # will drop the whole row with all the metrics
  gather(metric, value, 7:12) %>%
  
  # ==== Remove skipped words
  # mutate(zero = if_else(metric != "FPReg" & value == 0,T, F)) %>%
  # filter(zero == F) %>%
  
  # mutate(value = if_else(is.na(value), as.integer(0), as.integer(value))) %>%
  # mutate(value = if_else(metric != "FPReg" & is.na(value), as.integer(0), as.integer(value))) %>%
  drop_na() %>%
  mutate(word = str_trim(word)) %>%
  mutate(subj = str_remove(subj, "Sub")) %>%
  mutate(subj = as.integer(subj)) %>%
    group_by(text_id, word_text_idx, sent_id, word_idx, word, metric) %>%
    mutate(outlier = if_else(metric != "FPReg" & metric != "skip" & value > (mean(value) + 3 * sd(value) ), T, F)) %>%
    filter(outlier == F) %>%
  ungroup() #%>%

# Aggregate cross-participant data for all subjects
provo_eyetracking_agg_df = provo_eyetracking_df %>%
  group_by(text_id, word_text_idx, sent_id, word_idx, word, metric) %>%
    summarise(value = mean(value),
              nsubj = length(unique(subj))) %>%
    ungroup()

# View(provo_eyetracking_df)

# View(provo_eyetracking_agg_df)
# write.csv(provo_eyetracking_agg_df, file = "/Users/cui/Desktop/MoTR/pipeline/ancillary_data/provo_eyetracking_agg_df.csv", row.names = FALSE)

provo_raw_df %>%
  dplyr::select(IA_REGRESSION_OUT) %>%
  drop_na() %>%
  summarise( m = mean(IA_REGRESSION_OUT))

provo_raw_df %>%
  dplyr::select(IA_SKIP) %>%
  drop_na() %>%
  summarise( m = mean(IA_SKIP))


```

```{r}

# Split the eyetracking data in two by subjects to see how well it correlates with itself
provo_eyetracking_subj1_df_temp = provo_eyetracking_df %>%
  filter(subj <= 42) %>%
  mutate(word_text_idx = as.integer(word_text_idx - 1)) %>%
  group_by(text_id, word_text_idx, sent_id, word_idx, word, metric) %>%
    summarise(value = mean(value)) %>%
  ungroup() %>%
  rename(value_1 = value) #%>%
  # dplyr::select(-sent_id, -word_idx)

# View(provo_eyetracking_subj1_df_temp)

provo_eyetracking_subj1_df = merge(provo_eyetracking_subj1_df_temp, motr_agg_df, by=c("text_id", "word_text_idx", "metric")) %>%
  arrange(text_id, sent_id, word_idx) %>%
  filter(!(text_id == 13 & word_text_idx >= 20 & word_text_idx <= 52)) %>%
  filter(!(text_id == 3 & word_text_idx >= 46 & word_text_idx <= 57)) %>%
  rename(word = word.y) %>%
  dplyr::select(text_id, word_text_idx, metric, word, value_1)

# View(provo_eyetracking_subj1_df)

provo_eyetracking_subj2_df = provo_eyetracking_df %>%
  filter(subj > 42) %>%
  mutate(word_text_idx = as.integer(word_text_idx - 1)) %>%
  group_by(text_id, word_text_idx, sent_id, word_idx, word, metric) %>%
    summarise(value = mean(value)) %>%
  ungroup() %>%
    rename(value_2 = value)%>%
  dplyr::select(-sent_id, -word_idx)

# View(provo_eyetracking_subj2_df)
  
provo_eyetr_grouped_df = merge(provo_eyetracking_subj2_df, provo_eyetracking_subj1_df, by=c("text_id", "word_text_idx", "metric")) %>%
  # filter(word.x == word.y) %>%
  dplyr::select(-word.y) %>%
  group_by(metric) %>%
    mutate(motr_outlier = if_else(metric != "FPReg" & metric != "skip" & value_1 > (mean(value_1) + 3 * sd(value_1) ), T, F)) %>%
    filter(motr_outlier == F) %>%
    mutate(eyetr_outlier = if_else(metric != "FPReg" & metric != "skip" & value_2 > (mean(value_2) + 3 * sd(value_2) ), T, F)) %>%
    filter(eyetr_outlier == F) %>%
  ungroup() %>%
  gather(measure, value, c("value_1", "value_2")) %>%
  dplyr::select(-motr_outlier, -eyetr_outlier)

# View(provo_eyetr_grouped_df)

```


```{r}
provo_df = merge(provo_eyetracking_agg_df, provo_modeling_df, by=c("text_id", "sent_id", "word_idx")) %>%
  mutate(word_text_idx = as.integer(word_text_idx - 1)) %>%
  arrange(text_id, sent_id, word_idx) %>%
  rename(eyetr_value = value) 

provo_df = merge(provo_df, motr_agg_df, by=c("text_id", "word_text_idx", "metric")) %>%
arrange(text_id, sent_id, word_idx) %>%
  # almost all the word.x != word.y is because of normalization problem, so we can keep them, instead, deleting some special cases
filter(!(text_id == 13 & word_text_idx >= 20 & word_text_idx <= 52)) %>%
  filter(!(text_id == 3 & word_text_idx >= 46 & word_text_idx <= 57)) %>%
# filter(word.x == word) #%>%
dplyr::select(-word.x, -word.y) %>%
group_by(metric) %>%
  mutate(motr_outlier = if_else(metric != "FPReg" & motr_value > (mean(motr_value) + 3 * sd(motr_value) ), T, F)) %>%
  filter(motr_outlier == F) %>%
  mutate(eyetr_outlier = if_else(metric != "FPReg" & eyetr_value > (mean(eyetr_value) + 3 * sd(eyetr_value) ), T, F)) %>%
  filter(eyetr_outlier == F) %>%
ungroup() %>%
gather(measure, value, c("eyetr_value", "motr_value")) %>%
dplyr::select(-motr_outlier, -eyetr_outlier)
  
# View(provo_df)
# provo_df
```

```{r}
provo_df %>%
  mutate(measure = if_else(measure == "eyetr_value", "Eyetracking Value", "MoTR Value")) %>%
  filter(metric != "FPReg" & metric != "first_duration") %>%
  ggplot(aes(x = value, color=metric, fill=metric)) +
    geom_density(alpha = 0.01) +
    facet_grid(measure~.) +
    xlab("Reading Time (ms)") +
    coord_cartesian(xlim=c(0, 900)) +
    ylab("Density") +
    scale_y_continuous(breaks = c(0.001, 0.004)) +
  theme(
    legend.position = "none",
    axis.text.y = element_text(angle = 90, size = 7, hjust = 0.5)
  )

ggsave("../visualization/paper/density.pdf", device = "pdf", width = 2.2, height = 2.8)
```


```{r}
print("Gaze Duration")
gd_df = provo_df %>% filter(metric == "gaze_duration") %>% spread(measure, value)
print(cor.test(gd_df$eyetr_value, gd_df$motr_value)$estimate)

cor_df = provo_eyetr_grouped_df %>% filter(metric == "gaze_duration") %>% group_by(text_id, metric, measure) %>%
  summarize(value = mean(value, na.rm = TRUE), .groups = 'drop') %>% spread(measure, value)
print(cor.test(cor_df$value_1, cor_df$value_2)$estimate)

print("First Duration")
fd_df = provo_df %>% filter(metric == "first_duration") %>% spread(measure, value)
print(cor.test(fd_df$eyetr_value, fd_df$motr_value)$estimate)

# line 489, 490; 3147, 3148 share the same key
cor_df = provo_eyetr_grouped_df %>% filter(metric == "first_duration") %>% group_by(text_id, metric, measure) %>%
  summarize(value = mean(value, na.rm = TRUE), .groups = 'drop') %>% spread(measure, value)
print(cor.test(cor_df$value_1, cor_df$value_2)$estimate)

print("Go Past Time")

gpt_df = provo_df %>% filter(metric == "go_past_time") %>% spread(measure, value)
print(cor.test(gpt_df$eyetr_value, gpt_df$motr_value)$estimate)

cor_df = provo_eyetr_grouped_df %>% filter(metric == "go_past_time") %>% group_by(text_id, metric, measure) %>%
  summarize(value = mean(value, na.rm = TRUE), .groups = 'drop') %>% spread(measure, value)
print(cor.test(cor_df$value_1, cor_df$value_2)$estimate)

print("Total Duration")

td_df = provo_df %>% filter(metric == "total_duration") %>% spread(measure, value)
print(cor.test(td_df$eyetr_value, td_df$motr_value)$estimate)

cor_df = provo_eyetr_grouped_df %>% filter(metric == "total_duration") %>% group_by(text_id, metric, measure) %>%
  summarize(value = mean(value, na.rm = TRUE), .groups = 'drop') %>% spread(measure, value)
print(cor.test(cor_df$value_1, cor_df$value_2)$estimate)

print("Regression")

reg_df = provo_df %>% filter(metric == "FPReg") %>% spread(measure, value)
print(cor.test(reg_df$eyetr_value, reg_df$motr_value)$estimate)

cor_df = provo_eyetr_grouped_df %>% filter(metric == "FPReg") %>% group_by(text_id, metric, measure) %>%
  summarize(value = mean(value, na.rm = TRUE), .groups = 'drop') %>% spread(measure, value)
print(cor.test(cor_df$value_1, cor_df$value_2)$estimate)

#print("Skip")
#skip_df = provo_df %>% filter(metric == "skip") %>% spread(measure, value)
#print(cor.test(skip_df$eyetr_value, skip_df$motr_value)$estimate)

#cor_df = provo_eyetr_grouped_df %>% filter(metric == "skip") %>% group_by(text_id, metric, measure) %>%
  #summarize(value = mean(value, na.rm = TRUE), .groups = 'drop') %>% spread(measure, value)
#print(cor.test(cor_df$value_1, cor_df$value_2)$estimate)


```



```{r}

provo_df %>%
  mutate(metric = factor(metric, levels = c("first_duration", "gaze_duration", "go_past_time", "total_duration", "FPReg"),
                         labels = c("First Duration", "Gaze Duration", "Go Past Time", "Total Duration", "Regression Prob"))) %>%
  filter(metric != "Regression Prob" & metric != "First Duration") %>%
  spread(measure, value) %>%
  ggplot(aes(x = motr_value, y=eyetr_value, color =metric)) +
    geom_point(alpha = 0.05) +
    geom_abline(slope=1, intercept=0, color = "black") +
    facet_wrap(.~metric, scales = "free_x", nrow = 1) +
    coord_cartesian(ylim = c(0, 600), xlim = c(0, 800)) +
    geom_smooth(method = "lm", size = 0.7, linetype = "dashed", color = "black") +
  ylab("Eye Tracking Value") +
  xlab("MoTR Value") +
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  theme(
    legend.position = "right",
    legend.title = element_blank()
  )

ggsave("../visualization/paper/metric_cor.pdf", device = "pdf", width = 6, height = 2.5)
```
## Correlations to Word-Level Statistical Properties

```{r}
print("Len")
cor_df = provo_df %>% filter(metric == "gaze_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$len)$estimate)
print(cor.test(cor_df$motr_value, cor_df$len)$estimate)

print("Freq")
cor_df = provo_df %>% filter(metric == "gaze_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$freq)$estimate)
print(cor.test(cor_df$motr_value, cor_df$freq)$estimate)

print("Surp")
cor_df = provo_df %>% filter(metric == "gaze_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$surp)$estimate)
print(cor.test(cor_df$motr_value, cor_df$surp)$estimate)


```

```{r}
print("Len")
cor_df = provo_df %>% filter(metric == "total_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$len)$estimate)
print(cor.test(cor_df$motr_value, cor_df$len)$estimate)

print("Freq")
cor_df = provo_df %>% filter(metric == "total_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$freq)$estimate)
print(cor.test(cor_df$motr_value, cor_df$freq)$estimate)

print("Surp")
cor_df = provo_df %>% filter(metric == "total_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$surp)$estimate)
print(cor.test(cor_df$motr_value, cor_df$surp)$estimate)
```

```{r}
print("Len")
cor_df = provo_df %>% filter(metric == "first_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$len)$estimate)
print(cor.test(cor_df$motr_value, cor_df$len)$estimate)

print("Freq")
cor_df = provo_df %>% filter(metric == "first_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$freq)$estimate)
print(cor.test(cor_df$motr_value, cor_df$freq)$estimate)

print("Surp")
cor_df = provo_df %>% filter(metric == "first_duration") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$surp)$estimate)
print(cor.test(cor_df$motr_value, cor_df$surp)$estimate)
```

```{r}
print("Len")
cor_df = provo_df %>% filter(metric == "go_past_time") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$len)$estimate)
print(cor.test(cor_df$motr_value, cor_df$len)$estimate)

print("Freq")
cor_df = provo_df %>% filter(metric == "go_past_time") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$freq)$estimate)
print(cor.test(cor_df$motr_value, cor_df$freq)$estimate)

print("Surp")
cor_df = provo_df %>% filter(metric == "go_past_time") %>% spread(measure, value)
print(cor.test(cor_df$eyetr_value, cor_df$surp)$estimate)
print(cor.test(cor_df$motr_value, cor_df$surp)$estimate)
```



```{r}
provo_df %>%
  gather(word_prop, word_prop_val, c("freq", "len", "surp")) %>%
  mutate(measure = if_else(measure == "eyetr_value", "Eyetracking", "MoTR")) %>%
  mutate(word_prop = case_when(
    word_prop == "freq" ~ "Frequency (Log)",
    word_prop == "len" ~ "Length (Chars)",
    word_prop == "surp" ~ "Surprisal "
  )) %>%
  filter(metric == "gaze_duration") %>%
  mutate(label = paste(measure, word_prop, sep="\n")) %>%
  ggplot(aes(x = value, y=word_prop_val, color = word_prop)) +
    geom_point(alpha = 0.1) +
    facet_wrap(label~., scales="free_y", switch = "y", strip.position = "right", ncol = 3) +
    geom_smooth(method = "lm", color = "black", linetype = "dashed", size = 0.7) +
    xlab("MoTR Average Gaze Duration") +
    ylab("Word-Level Property") +
  theme(
    legend.position = "none",
    strip.placement = "outside",
    strip.background = element_blank(),
    strip.text = element_text(size = 8 )
  )

ggsave("../visualization/paper/word_prop_comps.pdf", device = "pdf", width = 6, height = 2.5)

```

```{r}
provo_df %>%
  ggplot(aes(x = value, y=freq, color=metric)) +
    geom_point(alpha = 0.1) +
    facet_grid(metric~measure, scales="free") +
    geom_smooth()
```

```{r}
provo_df %>%
  ggplot(aes(x = value, y=surp, color=metric)) +
    geom_point(alpha = 0.2) +
    facet_grid(metric~measure, scales="free") +
    geom_smooth()
```


## Shape of surprisal / RT relationship

```{r}

fit_gam_inner = function(bootstrap_sample, mean_predictors, target_prev_word) {
  
  df = bootstrap_sample$data
  weights = tabulate(as.integer(bootstrap_sample), nrow(df))
  
  m = gam(psychometric ~ s(surp, bs = 'cr', k = 6) + s(prev_surp, bs = 'cr', k = 6) + te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'), data = df, weights = weights)
  
  if(target_prev_word == F) {
    terms_to_predict = c("s(surp)", "s(prev_surp)")
    newdata = data.frame(surp=seq(0,20,by=0.1), prev_surp=mean_predictors$surp,
                         freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                         len=mean_predictors$len, prev_len=mean_predictors$len)
    # Returns a matrix N_samples * N_terms.
    per_term_predictions = predict(m, newdata=newdata, terms=terms_to_predict, type="terms")
  } else {
    terms_to_predict = c("s(surp)", "s(prev_surp)")
    newdata = data.frame(surp=mean_predictors$surp, prev_surp=seq(0,20,by=0.1),
                         freq=mean_predictors$freq, prev_freq=mean_predictors$freq,
                         len=mean_predictors$len, prev_len=mean_predictors$len)
    # Returns a matrix N_samples * N_terms.
    per_term_predictions = predict(m, newdata=newdata, terms=terms_to_predict, type="terms")
  }

  # Additive model -- sum across predictor response contributions (matrix columns).
  predictions = rowSums(per_term_predictions)
  
  # print("Fitted GAM model:")
  # print(m)

  # print("Predicted values for specified smooth terms:")
  # print(per_term_predictions)

  return(newdata %>% mutate(y=predictions))
}

fit_gam = function(df, mean_predictors, target_prev_word, alpha=0.05) {
  # Bootstrap-resample data
  boot_models = df %>% bootstraps(times=20) %>% 
   # Fit a GAM and get predictions for each sample
    mutate(smoothed=map(splits, fit_gam_inner, mean_predictors=mean_predictors, target_prev_word=target_prev_word))
  
  # Extract mean and 5% and 95% percentile y-values for each surprisal value
  if(target_prev_word == F) {
    
    result = boot_models %>% 
      unnest(smoothed) %>% 
      dplyr::select(surp, y) %>%
      group_by(surp) %>%
        summarise(y_lower=quantile(y, alpha / 2), 
                  y_upper=quantile(y, 1 - alpha / 2),
                  y=mean(y)) %>% 
      ungroup()
  } else {
    
    result = boot_models %>% 
      unnest(smoothed) %>% 
      dplyr::select(prev_surp, y) %>%
      group_by(prev_surp) %>%
        summarise(y_lower=quantile(y, alpha / 2), 
                  y_upper=quantile(y, 1 - alpha / 2),
                  y=mean(y)) %>%
      ungroup() %>%
      rename(surp = prev_surp)
  }
  # print(result)
  
  return (result)
}

```


```{r}

gam_modeling_df = provo_df %>%
  spread(measure, value) %>%
  # mutate(len = nchar(word)) %>%  # len has already exists, but do not count punct into len.
  group_by(metric, sent_id, text_id) %>%
    arrange(word_text_idx) %>%
    mutate(prev_surp = lag(surp),
           prev2_surp = lag(prev_surp),
           prev_freq = lag(freq),
           prev2_freq = lag(prev_freq),
           prev_len = lag(len),
           prev2_len = lag(prev_len),
           prev_eyetr_value = lag(eyetr_value)) %>%
  ungroup() %>%
  drop_na() %>%
  rename(psychometric = motr_value)
# View(gam_modeling_df)

gam_modeling_df %>%
  filter(metric == "gaze_duration") %>%
  arrange(text_id, sent_id, word_idx)

```

```{r}

smooths_df = data.frame()

metrics = c("gaze_duration", "total_duration", "go_past_time")
# metrics = c("gaze_duration")
for (m in metrics) {
  for( tval in c(T, F)) {
    print(paste0("Fitting model for ", m))
    dummy_df = gam_modeling_df %>% filter(metric == m)
    mean_predictors = dummy_df %>% summarise(surp = mean(surp), len = mean(len), freq = mean(freq))
    smooths = dummy_df %>% fit_gam(., mean_predictors, target_prev_word = tval)
    #Fix 0 surprisal = 0 mse
    gam_smooths = smooths %>% mutate(delta = 0 - y[1], y=y + delta, y_lower= y_lower + delta, y_upper=y_upper + delta)
    smooths_df = rbind(smooths_df, gam_smooths %>% mutate(psychometric = m, prev_word = tval))
    # View(smooths_df)
  }
}

```

### Get Density Data

```{r}
get_d_points = function(df) {
    x = density(df$surp)$x
    y = density(df$surp)$y
    return(data.frame(x, y))
  }

density_data = data.frame()

for(m in c("gaze_duration", "total_duration", "go_past_time")) {
  dummy_df = provo_df %>% filter(metric == m) %>%
      do({get_d_points(.)}) %>%
      filter(x>0, x<20)
  density_data = rbind(density_data, dummy_df %>% mutate(metric=m))
}

```


```{r}

plotting_df = smooths_df %>%
  mutate(target_word = if_else(prev_word == F, "wt", "wt-1"))


vnames <-list(
  "gaze_duration" = "Gaze Duration",
  "go_past_time" = "Go Past Time",
  "total_duration" = "Total Duration",
  'wt-1' = bquote(w[t-1]),
  'wt' = bquote(w[t])
)

vlabeller <- function(variable,value){
  return(vnames[value])
}

# Surprisal curves
  ggplot() +
      # Surrp / Rt data
      # geom_line(data = smooths_df, aes(x=prev_surp, y=y, color = psychometric), size=0.7) +
      geom_line(data = plotting_df, aes(x=surp, y=y, color = psychometric, linetype=target_word),  size=0.7) +
      # geom_ribbon(data = smooths_df, aes(x=prev_surp, ymin=y_lower, ymax=y_upper, fill = psychometric), alpha=0.3, size=0.5) +
      geom_ribbon(data = plotting_df, aes(x=surp, ymin=y_lower, ymax=y_upper, fill = psychometric), alpha=0.3, size=0.5) +
      # Density Data
      annotate("rect", xmin=-5, xmax=25, ymin=-25,ymax=-13, fill="white", color="grey", alpha=1, size = 0) +
      geom_line(data = density_data, aes(x=x, y=y*100 - 25), size = 0.2, color="#aaaaaa") +

      geom_ribbon(data = density_data, aes(x=x, ymin=-50, ymax=y*100 - 25), color="#dadeea", alpha = 0.1) +

      geom_hline(yintercept = -13, color = "black", size = 0.1) +
    
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
      facet_wrap(psychometric~target_word, nrow = 1, labeller = vlabeller) +
      ylab("Slowdown due to Surprisal (ms)") +
      xlab("Surprisal of Word") +
      coord_cartesian(ylim = c(-20, 100), xlim = c(0, 20)) +
      # ggtitle("MoTR Times and Previous Word Surprisal")
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank()
  )
  
ggsave("../visualization/paper/surp_rt_wt.pdf", device = "pdf", width = 6, height = 2.5)


```


## precision and recall for FPReg
```{r}
FPReg_df = provo_df %>% filter(metric == "FPReg") %>% spread(measure, value)
# write.csv(FPReg_df, file = "/Users/cui/Desktop/MoTR/pipeline/ancillary_data/FPReg.csv", row.names = FALSE)
confusion_matrix <- table(FPReg_df$motr_value > 0, FPReg_df$eyetr_value > 0)
confusion_matrix

true_positives <- confusion_matrix[2, 2]
false_positives <- confusion_matrix[2, 1]
false_negatives <- confusion_matrix[1, 2]

precision <- true_positives / (true_positives + false_positives)
recall <- true_positives / (true_positives + false_negatives)

print("precision of Motr FPReg:")
print(precision)
print("Recall of Motr FPReg:")
print(recall)
```



