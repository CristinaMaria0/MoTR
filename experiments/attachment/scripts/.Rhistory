pvalcond_1[i]<-summary(m_1)$coefficients[2,5]
}
}
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
## proportion of convergence failures
rowMeans(failed_1)
pvalcond_1[failed_1 == 1] <- NA
pow_1<-mean(pvalcond_1<= 0.05,na.rm=TRUE)
print(pow_1)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = (mean(failed) + mean(failed_1))/2,
power = ifelse(pow_1==0, pow, (pow + pow_1)/2)
)
}
power_df <- rbind(power_df, power_results)
}
}
}
}
rownames(power_df) <- NULL
write.csv(power_df, "../data/power_df_procedure23.csv")
# Power analysis for simulated number of participants (10, 20, 40, 60...200), while the number of items is always 24.
set.seed(234)
power_df = data.frame()
nsim <- 100
nitem <- 24
# for(exp in c("Coordination", "Relative Clause", "Adverb")) {
for(exp in c("Coordination")) {
# for(met in c("gaze_duration", "go_past_time", "total_duration")){
for(met in c("go_past_time")){
# for(mes in c("MoTR", "G-Maze", "A-Maze", "SPR")) {
for(mes in c("MoTR")) {
# for (nsubj in c(10, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200)) {
for (nsubj in c(140)) {
crit <- paste0("./bayesian_models_lc/model_2024_feb22_expo/", exp, "_",  met, "_", mes, "_0.rds")
crit_1 <- paste0("./bayesian_models_lc/model_2024_feb22_expo/", exp, "_",  met, "_", mes, "_1.rds")
pvalcond<-matrix(rep(NA,nsim),ncol=nsim)
failed<-matrix(rep(0,nsim),ncol=nsim)
if (mes != "SPR") {
model <- read_rds(crit)
model_summary <- summary(model)
effect_size <- model_summary$fixed["condition", "Estimate"]
for(i in 1:nsim){
fake_data <- gen_fake_df(nitem=nitem, nsubj=nsubj)
posterior_pred <- posterior_predict(model, newdata=fake_data, ndraws=1, allow_new_levels=T)
fake_data$rt <- t(posterior_pred)
## no correlations estimated to avoid convergence problems:
## analysis done after log-transforming:
m<-lmer(log(rt) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
summary(m)
# ## ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
failed[i]<-1
} else{
pvalcond[i]<-summary(m)$coefficients[2,5]
}
}
# print(pvalcond)
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
# print(pvalcond)
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = mean(failed),
power = pow
)
} else {
pvalcond_1<-matrix(rep(NA,nsim),ncol=nsim)
failed_1<-matrix(rep(0,nsim),ncol=nsim)
model <- read_rds(crit)
model_1 <- read_rds(crit_1)
model_summary <- summary(model)
effect_size <- model_summary$fixed["condition", "Estimate"]
for(i in 1:nsim){
fake_data <- gen_fake_df(nitem=nitem, nsubj=nsubj)
posterior_pred <- posterior_predict(model, newdata=fake_data, ndraws=1, allow_new_levels=T)
posterior_pred_1 <- posterior_predict(model_1, newdata=fake_data, ndraws=1, allow_new_levels=T)
fake_data$rt <- t(posterior_pred)
fake_data$rt_1 <- t(posterior_pred_1)
## no correlations estimated to avoid convergence problems:
## analysis done after log-transforming:
m<-lmer(log(rt) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
# summary(m)
m_1<-lmer(log(rt_1) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
# ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
failed[i]<-1
} else{
pvalcond[i]<-summary(m)$coefficients[2,5]
}
# for spillover region:
if(any( grepl("failed to converge", m_1@optinfo$conv$lme4$messages) )){
failed_1[i]<-1
} else{
pvalcond_1[i]<-summary(m_1)$coefficients[2,5]
}
}
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
## proportion of convergence failures
rowMeans(failed_1)
pvalcond_1[failed_1 == 1] <- NA
pow_1<-mean(pvalcond_1<= 0.05,na.rm=TRUE)
print(pow_1)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = (mean(failed) + mean(failed_1))/2,
power = ifelse(pow_1==0, pow, (pow + pow_1)/2)
)
}
power_df <- rbind(power_df, power_results)
}
}
}
}
rownames(power_df) <- NULL
write.csv(power_df, "../data/power_df_procedure23.csv")
# Power analysis for simulated number of participants (10, 20, 40, 60...200), while the number of items is always 24.
set.seed(234)
power_df = data.frame()
nsim <- 100
nitem <- 24
# for(exp in c("Coordination", "Relative Clause", "Adverb")) {
for(exp in c("Coordination")) {
# for(met in c("gaze_duration", "go_past_time", "total_duration")){
for(met in c("go_past_time")){
# for(mes in c("MoTR", "G-Maze", "A-Maze", "SPR")) {
for(mes in c("MoTR")) {
# for (nsubj in c(10, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200)) {
for (nsubj in c(140)) {
crit <- paste0("./bayesian_models_lc/model_2024_feb22_expo/", exp, "_",  met, "_", mes, "_0.rds")
crit_1 <- paste0("./bayesian_models_lc/model_2024_feb22_expo/", exp, "_",  met, "_", mes, "_1.rds")
pvalcond<-matrix(rep(NA,nsim),ncol=nsim)
failed<-matrix(rep(0,nsim),ncol=nsim)
if (mes != "SPR") {
model <- read_rds(crit)
model_summary <- summary(model)
effect_size <- model_summary$fixed["condition", "Estimate"]
for(i in 1:nsim){
fake_data <- gen_fake_df(nitem=nitem, nsubj=nsubj)
posterior_pred <- posterior_predict(model, newdata=fake_data, ndraws=1, allow_new_levels=T)
fake_data$rt <- t(posterior_pred)
## no correlations estimated to avoid convergence problems:
## analysis done after log-transforming:
m<-lmer(log(rt) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
summary(m)
# ## ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
failed[i]<-1
} else{
pvalcond[i]<-summary(m)$coefficients[2,5]
}
}
# print(pvalcond)
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
# print(pvalcond)
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = mean(failed),
power = pow
)
} else {
pvalcond_1<-matrix(rep(NA,nsim),ncol=nsim)
failed_1<-matrix(rep(0,nsim),ncol=nsim)
model <- read_rds(crit)
model_1 <- read_rds(crit_1)
model_summary <- summary(model)
effect_size <- model_summary$fixed["condition", "Estimate"]
for(i in 1:nsim){
fake_data <- gen_fake_df(nitem=nitem, nsubj=nsubj)
posterior_pred <- posterior_predict(model, newdata=fake_data, ndraws=1, allow_new_levels=T)
posterior_pred_1 <- posterior_predict(model_1, newdata=fake_data, ndraws=1, allow_new_levels=T)
fake_data$rt <- t(posterior_pred)
fake_data$rt_1 <- t(posterior_pred_1)
## no correlations estimated to avoid convergence problems:
## analysis done after log-transforming:
m<-lmer(log(rt) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
# summary(m)
m_1<-lmer(log(rt_1) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
# ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
failed[i]<-1
} else{
pvalcond[i]<-summary(m)$coefficients[2,5]
}
# for spillover region:
if(any( grepl("failed to converge", m_1@optinfo$conv$lme4$messages) )){
failed_1[i]<-1
} else{
pvalcond_1[i]<-summary(m_1)$coefficients[2,5]
}
}
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
## proportion of convergence failures
rowMeans(failed_1)
pvalcond_1[failed_1 == 1] <- NA
pow_1<-mean(pvalcond_1<= 0.05,na.rm=TRUE)
print(pow_1)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = (mean(failed) + mean(failed_1))/2,
power = ifelse(pow_1==0, pow, (pow + pow_1)/2)
)
}
power_df <- rbind(power_df, power_results)
}
}
}
}
rownames(power_df) <- NULL
write.csv(power_df, "../data/power_df_procedure23.csv")
# Power analysis for simulated number of participants (10, 20, 40, 60...200), while the number of items is always 24.
set.seed(123)
power_df = data.frame()
nsim <- 100
nitem <- 24
# for(exp in c("Coordination", "Relative Clause", "Adverb")) {
for(exp in c("Coordination")) {
# for(met in c("gaze_duration", "go_past_time", "total_duration")){
for(met in c("go_past_time")){
# for(mes in c("MoTR", "G-Maze", "A-Maze", "SPR")) {
for(mes in c("MoTR")) {
for (nsubj in c(10, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200)) {
# for (nsubj in c(140)) {
crit <- paste0("./bayesian_models_lc/model_2024_feb22_expo/", exp, "_",  met, "_", mes, "_0.rds")
crit_1 <- paste0("./bayesian_models_lc/model_2024_feb22_expo/", exp, "_",  met, "_", mes, "_1.rds")
pvalcond<-matrix(rep(NA,nsim),ncol=nsim)
failed<-matrix(rep(0,nsim),ncol=nsim)
if (mes != "SPR") {
model <- read_rds(crit)
model_summary <- summary(model)
effect_size <- model_summary$fixed["condition", "Estimate"]
for(i in 1:nsim){
fake_data <- gen_fake_df(nitem=nitem, nsubj=nsubj)
posterior_pred <- posterior_predict(model, newdata=fake_data, ndraws=1, allow_new_levels=T)
fake_data$rt <- t(posterior_pred)
## no correlations estimated to avoid convergence problems:
## analysis done after log-transforming:
m<-lmer(log(rt) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
summary(m)
# ## ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
failed[i]<-1
} else{
pvalcond[i]<-summary(m)$coefficients[2,5]
}
}
# print(pvalcond)
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
# print(pvalcond)
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = mean(failed),
power = pow
)
} else {
pvalcond_1<-matrix(rep(NA,nsim),ncol=nsim)
failed_1<-matrix(rep(0,nsim),ncol=nsim)
model <- read_rds(crit)
model_1 <- read_rds(crit_1)
model_summary <- summary(model)
effect_size <- model_summary$fixed["condition", "Estimate"]
for(i in 1:nsim){
fake_data <- gen_fake_df(nitem=nitem, nsubj=nsubj)
posterior_pred <- posterior_predict(model, newdata=fake_data, ndraws=1, allow_new_levels=T)
posterior_pred_1 <- posterior_predict(model_1, newdata=fake_data, ndraws=1, allow_new_levels=T)
fake_data$rt <- t(posterior_pred)
fake_data$rt_1 <- t(posterior_pred_1)
## no correlations estimated to avoid convergence problems:
## analysis done after log-transforming:
m<-lmer(log(rt) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
# summary(m)
m_1<-lmer(log(rt_1) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
# ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
failed[i]<-1
} else{
pvalcond[i]<-summary(m)$coefficients[2,5]
}
# for spillover region:
if(any( grepl("failed to converge", m_1@optinfo$conv$lme4$messages) )){
failed_1[i]<-1
} else{
pvalcond_1[i]<-summary(m_1)$coefficients[2,5]
}
}
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
## proportion of convergence failures
rowMeans(failed_1)
pvalcond_1[failed_1 == 1] <- NA
pow_1<-mean(pvalcond_1<= 0.05,na.rm=TRUE)
print(pow_1)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = (mean(failed) + mean(failed_1))/2,
power = ifelse(pow_1==0, pow, (pow + pow_1)/2)
)
}
power_df <- rbind(power_df, power_results)
}
}
}
}
rownames(power_df) <- NULL
write.csv(power_df, "../data/power_df_procedure23.csv")
# Power analysis for simulated number of participants (10, 20, 40, 60...200), while the number of items is always 24.
set.seed(4444)
power_df = data.frame()
nsim <- 100
nitem <- 24
# for(exp in c("Coordination", "Relative Clause", "Adverb")) {
for(exp in c("Coordination")) {
# for(met in c("gaze_duration", "go_past_time", "total_duration")){
for(met in c("go_past_time")){
# for(mes in c("MoTR", "G-Maze", "A-Maze", "SPR")) {
for(mes in c("MoTR")) {
for (nsubj in c(10, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200)) {
# for (nsubj in c(140)) {
crit <- paste0("./bayesian_models_lc/model_2024_feb22_expo/", exp, "_",  met, "_", mes, "_0.rds")
crit_1 <- paste0("./bayesian_models_lc/model_2024_feb22_expo/", exp, "_",  met, "_", mes, "_1.rds")
pvalcond<-matrix(rep(NA,nsim),ncol=nsim)
failed<-matrix(rep(0,nsim),ncol=nsim)
if (mes != "SPR") {
model <- read_rds(crit)
model_summary <- summary(model)
effect_size <- model_summary$fixed["condition", "Estimate"]
for(i in 1:nsim){
fake_data <- gen_fake_df(nitem=nitem, nsubj=nsubj)
posterior_pred <- posterior_predict(model, newdata=fake_data, ndraws=1, allow_new_levels=T)
fake_data$rt <- t(posterior_pred)
## no correlations estimated to avoid convergence problems:
## analysis done after log-transforming:
m<-lmer(log(rt) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
summary(m)
# ## ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
failed[i]<-1
} else{
pvalcond[i]<-summary(m)$coefficients[2,5]
}
}
# print(pvalcond)
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
# print(pvalcond)
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = mean(failed),
power = pow
)
} else {
pvalcond_1<-matrix(rep(NA,nsim),ncol=nsim)
failed_1<-matrix(rep(0,nsim),ncol=nsim)
model <- read_rds(crit)
model_1 <- read_rds(crit_1)
model_summary <- summary(model)
effect_size <- model_summary$fixed["condition", "Estimate"]
for(i in 1:nsim){
fake_data <- gen_fake_df(nitem=nitem, nsubj=nsubj)
posterior_pred <- posterior_predict(model, newdata=fake_data, ndraws=1, allow_new_levels=T)
posterior_pred_1 <- posterior_predict(model_1, newdata=fake_data, ndraws=1, allow_new_levels=T)
fake_data$rt <- t(posterior_pred)
fake_data$rt_1 <- t(posterior_pred_1)
## no correlations estimated to avoid convergence problems:
## analysis done after log-transforming:
m<-lmer(log(rt) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
# summary(m)
m_1<-lmer(log(rt_1) ~ 1 + condition + (condition || subj) + (condition || item_id),
data=fake_data)
# ignore failed trials
if(any( grepl("failed to converge", m@optinfo$conv$lme4$messages) )){
failed[i]<-1
} else{
pvalcond[i]<-summary(m)$coefficients[2,5]
}
# for spillover region:
if(any( grepl("failed to converge", m_1@optinfo$conv$lme4$messages) )){
failed_1[i]<-1
} else{
pvalcond_1[i]<-summary(m_1)$coefficients[2,5]
}
}
## proportion of convergence failures:
rowMeans(failed)
pvalcond[failed == 1] <- NA
pow<-mean(pvalcond<= 0.05,na.rm=TRUE)
print(pow)
## proportion of convergence failures
rowMeans(failed_1)
pvalcond_1[failed_1 == 1] <- NA
pow_1<-mean(pvalcond_1<= 0.05,na.rm=TRUE)
print(pow_1)
power_results <- data.frame(
experiment = exp,
metric = met,
measure = mes,
n_subj = nsubj,
n_item = nitem,
effect_size = effect_size,
failed_mean = (mean(failed) + mean(failed_1))/2,
power = ifelse(pow_1==0, pow, (pow + pow_1)/2)
)
}
power_df <- rbind(power_df, power_results)
}
}
}
}
rownames(power_df) <- NULL
write.csv(power_df, "../data/power_df_procedure24.csv")
# Plot power analysis
all_power <- read.csv("../data/power_df_procedure2.csv") %>%
filter(metric == 'go_past_time') %>%
filter(n_subj <= 200) %>%
dplyr::select(experiment, metric, measure, n_subj, power)
all_power
ggplot(all_power)+
geom_point(aes(x=n_subj, y=power, color=measure), size=1.8)+
geom_line(aes(x=n_subj, y=power, color=measure), size=1)+
facet_grid(~experiment)+
geom_hline(yintercept=.8)+
coord_cartesian(xlim=c(0,200))+
labs(y="Power estimate", x="Simulated participant count")+
theme_bw()+
theme(
legend.position="right",
legend.title=element_blank(),
text = element_text(size = 13, colour="black"))+
scale_color_manual(values=c(rgb(139/255, 173/255, 51/255),rgb(172/255, 222/255, 225/255) , rgb(231/255, 125/255, 144/255), rgb(189/255, 127/255, 248/255))) +
scale_x_continuous(breaks=seq(0, 200, by=40))
ggsave(paste0("../visualizations/power_analysis_procedure2", ".pdf"), device="pdf", width = 8, height = 3)
